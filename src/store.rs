use lazy_static::lazy_static;
use serde::Serialize;
use std::{
    ffi::OsStr,
    fs::{remove_dir_all, File, OpenOptions},
    io::Write,
    marker::PhantomData,
    path::{Path, PathBuf},
    sync::{mpsc, Arc, Mutex},
    thread::{self, JoinHandle},
};

use chrono::{DateTime, Utc};
use flate2::{write::GzEncoder, Compression};

/// Binary data version string.
pub const BINARY_VERSION: &str = "1.0";
const BINARY_ALIGN: u32 = size_of::<u32>() as _;
const BINARY_PADDING: [u8; BINARY_ALIGN as usize] = [0xff; BINARY_ALIGN as usize];
const BINARY_FRAME_START: &[u8] = b"FRME";

/// This trait contains functions that describe the file extension,
/// inter-frame delimiter and header initializer for a data storage
/// type used by [StoreCfg].
/// [StoreCfg] only works with data types that implement this trait.
pub trait FmtInfo {
    /// Delimiter between each frame
    fn delimiter() -> &'static [u8];
    /// File extension of frame
    fn extension() -> &'static str;
    /// Initialize a new file
    ///
    /// # Arguments
    /// - `writer`: Where header data is written
    /// - `progname`: Program that is writing this data file
    fn initialize<W>(writer: W, progname: &str) -> std::io::Result<()>
    where
        W: Write;
}

#[derive(Debug)]
/// Binary data store marker for [StoreCfg].
pub struct Binary {}

impl FmtInfo for Binary {
    fn delimiter() -> &'static [u8] {
        b""
    }

    fn extension() -> &'static str {
        "bin"
    }

    fn initialize<W: Write>(writer: W, progname: &str) -> std::io::Result<()> {
        let fmt = format!(
            "
# This file is generated by {} using binary format version {}.
# The binary file is aligned to 4 byte boundaries.
# The file format is described as follows:
# <Ofst 0: Frame size (4 LE bytes, aligned to 4 bytes)>
# <Ofst 4: Payload size (4 LE bytes)>
# <Ofst 8: Data[0..Payload Size + 1]>
# <Ofst Payload Size + 1: Padding[FF; Payload Size % 4]>
# <Next Frame Start>\n",
            progname, BINARY_VERSION
        );
        store_binary(writer, fmt.as_bytes())
    }
}

#[derive(Debug)]
/// JSONL data store marker for [StoreCfg].
pub struct Json<T> {
    _marker: PhantomData<T>,
}

#[derive(Debug, Serialize)]
/// JSONL data file header.
pub struct JsonHeader {
    header: String,
}

impl JsonHeader {
    fn new(progname: &str) -> Self {
        Self {
            header: format!("This file is created by {progname}."),
        }
    }
}

impl<T> FmtInfo for Json<T> {
    fn delimiter() -> &'static [u8] {
        b"\n"
    }

    fn extension() -> &'static str {
        "Json"
    }

    fn initialize<W: Write>(mut writer: W, progname: &str) -> std::io::Result<()> {
        let repr = serde_json::to_string(&JsonHeader::new(progname))
            .map_err(|err| std::io::Error::new(std::io::ErrorKind::InvalidInput, err))?;
        writer.write_all(repr.as_bytes())?;
        writer.write_all(b"\n")?;
        writer.flush()?;
        Ok(())
    }
}

impl<T> Drop for StoreCfg<T> {
    fn drop(&mut self) {
        if let Some(tx) = &self.compress_tx {
            if let Some(hdl) = self.compress_hdl.take() {
                let _ = tx.send(None);
                let _ = hdl.join();
            }
        }
    }
}

#[derive(Debug)]
/// Data storage configuration of some type. Currently, the type
/// is either [Binary] or [Json].
///
/// This struct is used to store data frames in a directory
/// structure relative to the root as follows:
/// /path/to/root/YYYYMMDD/YYYYMMDDHHMM.{EXTENSION}
///
/// New files are created every UTC hour, and over the hour
/// appends the data packets into the file.
///
/// If compression is enabled, the individual hourly files
/// are compressed into a tarball at the end of the day, as
/// /path/to/root/YYYYMMDD.tar.gz. The directory YYYYMMDD
/// is removed after successful compression.
///
/// Usage:
/// ```rust,no_run
/// use datastor::{StoreCfg, Binary, Json};
/// use chrono::{Utc, Duration};
/// let mut store = StoreCfg::<Binary>::new("test".into(), true, "testprogram").unwrap();
/// let data = vec![1, 2, 3, 4, 5];
/// let now = Utc::now();
/// let _ = store.store(now, data.as_ref()).unwrap(); // first frame
/// let _ = store.store(now + Duration::hours(2), data.as_ref()).unwrap(); // second frame
/// let _ = store.store(now + Duration::hours(25), data.as_ref()).unwrap(); // third frame, this will trigger a compression event
///
pub struct StoreCfg<Kind> {
    root_dir: PathBuf,
    current_dir: PathBuf,
    last_date: Option<String>,
    last_hour: Option<String>,
    writer: Option<File>,
    filename: PathBuf,
    compress_tx: Option<mpsc::Sender<Option<PathBuf>>>,
    compress_hdl: Option<thread::JoinHandle<()>>,
    progname: &'static str,
    _marker: PhantomData<Kind>,
}

impl<Kind> StoreCfg<Kind> {
    #[must_use = "The errors must be handled."]
    /// Create a new storage configuration.
    ///
    /// # Arguments:
    /// - `root_dir`: Root directory where data files will be stored.
    /// - `compress`: Whether individual, hourly files will be compressed into a tarball at the end of the day.
    /// - `progname`: Name of the program creating this data file.
    pub fn new(
        root_dir: PathBuf,
        compress: bool,
        progname: &'static str,
    ) -> Result<Self, std::io::Error> {
        std::fs::create_dir_all(&root_dir)?;
        lazy_static! {
            static ref COMPRESSION_THREAD_TX: Arc<Mutex<Option<mpsc::Sender<Option<PathBuf>>>>> =
                Arc::new(Mutex::new(None));
        }
        // handle compression
        let (compress_tx, compress_hdl) = if compress {
            // if compressing
            if let Ok(mut tx) = COMPRESSION_THREAD_TX.lock() {
                if let Some(tx) = tx.as_ref() {
                    // already initialized
                    (Some(tx.clone()), None) // return the sender
                } else {
                    // we need to initialize the compression thread
                    let (ctx, rx) = mpsc::channel(); // create a channel
                    *tx = Some(ctx.clone()); // store the sender in the mutex
                                             // spawn the compression thread
                    let hdl = compressor(rx);
                    (Some(ctx), Some(hdl)) // return the sender
                }
            } else {
                (None, None)
            }
        } else {
            // if not compressing
            (None, None)
        };
        Ok(Self {
            root_dir,
            current_dir: PathBuf::new(),
            last_date: None,
            last_hour: None,
            writer: None,
            filename: PathBuf::new(),
            compress_tx,
            compress_hdl,
            progname,
            _marker: PhantomData,
        })
    }
}

impl<Kind: FmtInfo> StoreCfg<Kind> {
    #[must_use = "Errors must be handled."]
    fn check_time(&mut self, tstamp: DateTime<Utc>) -> Result<(&Path, &mut File), std::io::Error> {
        let date = tstamp.format("%Y%m%d").to_string();
        let hour = tstamp.format("%H").to_string();
        if self.last_date.as_deref() != Some(&date) {
            // Send the last directory to the compression thread
            if let Some(tx) = &self.compress_tx {
                let _ = tx.send(Some(self.current_dir.clone()));
            }
            self.current_dir = self.root_dir.join(&date);
            std::fs::create_dir_all(&self.current_dir)?;
            self.last_date = Some(date.clone());
            self.last_hour = None;
        }
        if self.last_hour.as_deref() != Some(&hour) {
            let filename =
                self.current_dir
                    .join(format!("{}{}0000.{}", &date, &hour, Kind::extension()));
            let mut writer = if filename.exists() {
                OpenOptions::new().append(true).open(&filename)
            } else {
                File::create(&filename)
            }?;
            Kind::initialize(&mut writer, self.progname)?;
            self.writer = Some(writer);
            self.last_hour = Some(hour);
            self.filename = filename;
        };
        let filename = self.filename.as_path();
        Ok((filename, self.writer.as_mut().unwrap())) // Safety: At this point, self.writer MUST be populated.
    }
}

impl<T: Serialize> StoreCfg<Json<T>> {
    #[must_use = "Errors must be handled."]
    /// Store a JSON-serializable data frame.
    ///
    /// # Arguments:
    /// - `tstamp`: Timestamp of the data frame, used to determine hourly and daily boundaries.
    /// - `data`: Data to be stored, must implement [serde::Serialize].
    ///
    /// # Output:
    /// - Returns the path to the file where the data was stored.
    /// 
    /// # Errors:
    /// - If the data cannot be serialized to JSON.
    /// - If the file cannot be opened or written to.
    /// - If the file cannot be flushed.
    pub fn store(&mut self, tstamp: DateTime<Utc>, data: &T) -> Result<&Path, std::io::Error>
    where
        T: serde::Serialize,
    {
        let (filename, writer) = self.check_time(tstamp)?;
        serde_json::to_writer(writer.try_clone()?, &data)
            .map_err(|err| std::io::Error::new(std::io::ErrorKind::InvalidInput, err))?;
        writer.write_all(Json::<T>::delimiter())?;
        writer.flush()?;
        Ok(filename)
    }
}

impl StoreCfg<Binary> {
    #[must_use = "Errors must be handled."]
    /// Store a binary data frame.
    ///
    /// # Arguments:
    /// - `tstamp`: Timestamp of the data frame, used to determine hourly and daily boundaries.
    /// - `data`: Data to be stored, must be a byte slice.
    /// 
    /// # Output:
    /// - Returns the path to the file where the data was stored.
    ///
    /// # Errors:
    /// - If the data is too large (greater than 4 GiB).
    /// - If the file cannot be opened or written to.
    /// - If the file cannot be flushed.
    pub fn store(&mut self, tstamp: DateTime<Utc>, data: &[u8]) -> Result<&Path, std::io::Error> {
        let (filename, writer) = self.check_time(tstamp)?;
        let data_size = data.len();
        if data_size > u32::MAX as _ {
            return Err(std::io::Error::new(
                std::io::ErrorKind::QuotaExceeded,
                "Input data too large, must be < 4GB",
            ));
        }
        store_binary(writer, data)?;
        Ok(filename)
    }
}

fn store_binary<W>(mut writer: W, data: &[u8]) -> Result<(), std::io::Error>
where
    W: Write,
{
    let data_size = data.len() as u32;
    let padding = BINARY_ALIGN - (data_size % BINARY_ALIGN);
    let frame_size = data_size
        .checked_add(size_of::<u32>() as u32 + padding) // payload size + padding
        .ok_or(std::io::Error::new(
            std::io::ErrorKind::OutOfMemory,
            "Frame size too large",
        ))?;
    debug_assert!(
        frame_size % BINARY_ALIGN == 0,
        "Frame size {frame_size} is not {BINARY_ALIGN}-byte aligned: {data_size} + {padding}"
    );
    writer.write_all(BINARY_FRAME_START)?; // write frame start
    writer.write_all(&frame_size.to_le_bytes())?; // write frame size
    writer.write_all(&data_size.to_le_bytes())?; // write header size
    writer.write_all(data)?; // write the data
    writer.write_all(&BINARY_PADDING[..padding as usize])?; // write the padding
    writer.flush()?;
    Ok(())
}

fn compressor(rx: mpsc::Receiver<Option<PathBuf>>) -> JoinHandle<()> {
    thread::spawn(move || {
        log::info!("Compression thread started");
        while let Ok(last_dir) = rx.recv() {
            if let Some(last_dir) = last_dir {
                // wait for a directory to compress
                let mut outfile = last_dir.clone(); // create the output file
                outfile.set_extension("tar.gz");
                log::info!("Compressing {last_dir:?} to {outfile:?}...");
                if let Ok(outfile) = File::create(outfile) {
                    // create the output file
                    let tar = GzEncoder::new(outfile, Compression::default()); // create the gzip encoder
                    let mut tar = tar::Builder::new(tar); // create the tar builder
                    let res = if last_dir.is_dir() {
                        // if the input is a directory
                        let root = last_dir.file_name().unwrap_or(OsStr::new(".")); // get the root directory
                        tar.append_dir_all(root, &last_dir) // append the directory to the tar
                    } else {
                        // if the input is a file
                        tar.append_path(&last_dir) // append the file to the tar
                    };
                    match res {
                        // check the result
                        Ok(_) => {
                            // if successful
                            if let Err(e) = remove_dir_all(&last_dir) {
                                // delete the input directory
                                log::warn!("Error deleting directory {last_dir:?}: {e:?}");
                            } else {
                                log::info!("Compression successful! Deleted {last_dir:?}");
                            }
                        }
                        Err(e) => {
                            // if there was an error
                            log::warn!("Compression error {e:?}: {last_dir:?}");
                        }
                    }
                }
            } else {
                break;
            }
        }
        log::info!("Compression thread exiting");
    })
}
